# -*- coding: utf-8 -*-
"""BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f0IGzmcn8tVrPLWYnSbmXcx1V3iZIg0p
"""

from google.colab import drive
drive.mount('/content/drive/')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/nlp/relation_extraction_colab
# %shell pip install -q -U transformers

from process_data import RE_DataEncoder
from BaseModel import BaseModel
from generate_bert_data import preprocess_data_BERT

import numpy as np
import pandas as pd
import seaborn as sns
import pickle
from transformers import TFBertModel

from keras.layers import Dense, Embedding, Conv1D

from tensorflow.keras.layers import Embedding, Dense, Dropout, Input, concatenate, GlobalMaxPool1D
from keras.models import Model

from tensorflow.keras.metrics import CategoricalAccuracy
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.optimizers import Adam

def gen_glove_vector():
    with open('glove.6B.300d.txt', 'r', encoding='UTF-8') as f:
        words = set()
        word_to_vec_map = {}
        for line in f:
            w_line = line.split()
            curr_word = w_line[0]
            word_to_vec_map[curr_word] = np.array(w_line[1:], dtype=np.float64)
    emb_matrix = np.zeros((Encoder.word_size+1, 300)) # vì word_index nó không quan tâm đến num_words đã set, nên 
    for word, index in Encoder.word_index.items():
        embedding_vector = word_to_vec_map.get(word)
        if embedding_vector is not None:
            emb_matrix[index, :] = embedding_vector
    return emb_matrix

with open('data/data_encoder.obj', 'rb') as f:
    Encoder=pickle.load(f)

vocab_size=Encoder.vocab_size
max_len= Encoder.max_len
emb_matrix=gen_glove_vector()

X_train = np.load('data/X_train.npy')
X_test = np.load('data/X_test.npy')
y_train = np.load('data/y_train.npy')
y_test = np.load('data/y_test.npy')

bert_transformer = TFBertModel.from_pretrained('bert-base-cased')

# Preprocessing data
train = pd.read_csv('data/train.csv')
test = pd.read_csv('data/test.csv')

def empty_matrix(data, max_len=max_len):
    return np.zeros((len(data), max_len))

train_dataset = preprocess_data_BERT(train, X_train, y_train).generate_data(empty_matrix(train), empty_matrix(train))
test_dataset = preprocess_data_BERT(test, X_test, y_test).generate_data(empty_matrix(test), empty_matrix(test))

class BERT_CNN(BaseModel):
    def build_model(self, using=['word_emb', 'position_emb', 'gram_emb', 'sp_emb']):
        input_sentence = Input(shape=(max_len,), name='sentence')

        embed_sentence_glove = Embedding(input_dim=Encoder.word_size+1, 
                                    output_dim=300, #cần tương thích vs tham số weights bên dưới
                                    input_length=max_len,
                                    weights = [emb_matrix],
                                    name='sentence_glove', mask_zero=True)(input_sentence)

        input_e1_pos = Input(shape=(max_len,), name='e1_position')
        embed_e1_pos = Embedding(126,200, input_length=max_len, mask_zero=True)(input_e1_pos)
        input_e2_pos = Input(shape=(max_len,), name='e2_position')
        embed_e2_pos = Embedding(122,200, input_length=max_len, mask_zero=True)(input_e2_pos)

        input_grammar = Input(shape=(max_len,), name='grammar_relation')
        embed_grammar = Embedding(45,100, input_length=max_len, mask_zero=True)(input_grammar)

        input_sp = Input(shape=(max_len,), name='shortest_path')
        embed_sp = Embedding(62, 500, input_length=max_len, mask_zero=True)(input_sp)

        input_ids = Input(shape=(max_len,), name='input_ids', dtype='int32')
        attn_masks = Input(shape=(max_len,), name='attention_mask', dtype='int32')

        input_list=[]
        if 'word_emb' in using:
            input_list.append(embed_sentence_glove)
        if 'position_emb' in using:
            input_list.extend([embed_e1_pos, embed_e2_pos])
        if 'gram_emb' in using:
            input_list.append(embed_grammar)
        if 'sp_emb' in using:
            input_list.append(embed_sp)
        
        bert_embds = bert_transformer.bert(input_ids, attn_masks)[0] # 0 -> activation layer (3D), 1 -> pooled output layer (2D)
        bert_inter = Dense(128, activation='relu', name='intermediate_bert')(bert_embds)
        
        visible = concatenate([bert_inter, embed_e1_pos, embed_e2_pos, embed_grammar, embed_sp])
        interp = Conv1D(filters=194, kernel_size=3, activation='relu')(visible)
        interp = GlobalMaxPool1D()(interp)
        output = Dense(19, activation='softmax')(interp)
        self.model = Model(inputs=[input_ids, attn_masks, input_e1_pos, input_e2_pos, input_grammar, input_sp], outputs=output)
    
    def train_model(self, train_dataset, epochs=1):
        optim = Adam(learning_rate=1e-5)
        loss_func = CategoricalCrossentropy()
        acc = CategoricalAccuracy('accuracy')
        self.model.compile(optimizer=optim, loss=loss_func, metrics=[acc])
        history = self.model.fit(
            train_dataset, 
            epochs=epochs
        )

#CNN 
bert_cnn_model=BERT_CNN()
bert_cnn_model.build_model()
bert_cnn_model.train_model(train_dataset)
bert_cnn_model.evaluate(X_test, y_test, Encoder.dict_labels)
bert_cnn_model.save_model('bert_cnn')

# cnn_model=CNN_model()
# cnn_model.load_model('cnn_nonbert')
# cnn_model.evaluate(X_test, y_test, Encoder.dict_labels)